<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Airbnb Case Study</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="Airbnb_case_study.css">
</head>

<body>
    <div class="container">
        <section id="introduction">
            <h2>üß≠ Section 1: Introduction and Motivation</h2>

            <p>
                Personalized search lies at the heart of Airbnb's user experience. To deliver fast, accurate, and real-time recommendations to millions of users across a petabyte-scale dataset, Airbnb needed more than a conventional database‚Äîit needed a storage engine built around <strong>write-optimized data structures</strong>, efficient compaction strategies, and SSD-aware caching layers.
            </p>

            <p>
                This section explores the architectural evolution that led Airbnb to adopt <strong>RocksDB</strong> and build its own key-value store, <strong>Mussel</strong>, to power personalized search at scale.
            </p>

            <h3>üéØ The Use Case</h3>
            <p>
                Airbnb's goal was to build a search engine capable of <strong>real-time, per-user query execution</strong> over billions of listings, impressions, and clickstream signals‚Äîall stored in key-value form. The system needed to support:
            </p>
            <ul>
                <li>Ultra-fast writes for user events (up to <code>1M events/sec</code>)</li>
                <li>Low-latency reads (<code>p99 &lt; 100ms</code>)</li>
                <li>Efficient caching and disk layout strategies</li>
            </ul>

            <h3>‚ùå Why Traditional Systems Failed</h3>
            <p>
                Early experiments with <strong>MySQL</strong> and <strong>HBase</strong> quickly revealed critical shortcomings:
            </p>
            <ul>
                <li>Random I/O for writes caused bottlenecks</li>
                <li>Tail latencies exceeded SLA targets under load</li>
                <li>Lack of control over compaction scheduling and caching</li>
                <li>Poor support for fine-grained per-user partitioning</li>
            </ul>

            <figure>
                <img src="./assets/mussel_architecture.png" alt="Airbnb Mussel Architecture Diagram" style="width:100%; max-width:700px;">
                <figcaption>
                    <strong>Figure 1:</strong> Architecture of Mussel ‚Äì Airbnb's custom RocksDB-based key-value store powering personalized search. <em>Source: Airbnb Engineering Blog</em>.
                </figcaption>
            </figure>

            <p>
                Airbnb responded by building <strong>Mussel</strong>‚Äîa modular, embedded key-value engine based on <a href="https://github.com/facebook/rocksdb" target="_blank">RocksDB</a>. It offered the configurability, write throughput, and compaction control that generalized systems lacked.
            </p>

            <h3>‚úÖ Foundation: Log-Structured Merge Trees (LSM Trees)</h3>
            <p>
                RocksDB relies on a foundational data structure known as the <strong>Log-Structured Merge Tree (LSM Tree)</strong>, originally described by O'Neil et al. in their 1996 paper. Unlike B-Trees, LSM Trees batch inserts in memory and flush them to disk in sorted order, enabling:
            </p>
            <ul>
                <li><strong>Amortized O(log N)</strong> write cost</li>
                <li>Efficient merge-based compaction</li>
                <li>High sequential write throughput on SSDs</li>
            </ul>

            <figure>
                <img src="./assets/LSM_leveled_compactions.png" alt="LSM Tree with Leveled Compaction" style="width:100%; max-width:650px;">
                <figcaption>
                    <strong>Figure 2:</strong> RocksDB's LSM Tree layout with <em>Leveled Compaction</em>. New SSTables are flushed from memory to disk and later merged level-by-level. <em>Source: RocksDB Wiki</em>.
                </figcaption>
            </figure>

            <p>
                This structure is ideal for Airbnb's use case where writes outnumber reads and disk I/O must be minimized. The LSM Tree also enabled Airbnb to tune compaction styles‚Äîchoosing between <strong>leveled</strong>, <strong>universal</strong>, and <strong>FIFO</strong> strategies depending on access patterns.
            </p>

            <h3>üß† Why RocksDB Was a Game Changer</h3>
            <p>
                According to <em>RocksDB: Evolution of Development Priorities</em> (Dong et al., 2021), RocksDB allowed application owners to:
            </p>
            <ul>
                <li>Tune <strong>write amplification</strong> vs. <strong>read efficiency</strong></li>
                <li>Optimize <strong>cache locality</strong> with block-level caching</li>
                <li>Control <strong>compaction rate</strong> and <strong>disk layout</strong> at column-family level</li>
            </ul>

            <figure>
                <img src="assets/amplication_vs_compaction.png" alt="Write amplification vs compaction style" style="width:100%; max-width:680px;">
                <figcaption>
                    <strong>Figure 3:</strong> Write amplification vs. compaction style. Airbnb used <em>Universal Compaction</em> for faster writes at the cost of space efficiency. <em>Source: ACM digital library (DL) <a href="https://dl.acm.org/doi/10.1145/3483840">https://dl.acm.org/doi/10.1145/3483840</a></em>.
                </figcaption>
            </figure>

            <h3>üîç Read Optimization: Bloom Filters</h3>
            <p>
                Read paths in RocksDB are accelerated using <strong>Bloom Filters</strong>, a probabilistic data structure that checks set membership in <code>O(k)</code> time with minimal space. RocksDB associates Bloom Filters with each SSTable to skip unnecessary disk I/O during point lookups.
            </p>

            <figure>
                <img src="assets/sbf.png" alt="LSM Tree Layers with Bloom Filters" style="width:100%; max-width:680px;">
                <figcaption>
                    <strong>Figure 4:</strong> Bloom Filters used with each SSTable to skip reading tables that don't contain the queried key. <em>Source: O'Neil et al., LSM Tree paper</em>.
                </figcaption>
            </figure>

            <p>
                Persistent Bloom Filters (PBF), as explored by Peng et al. in SIGMOD 2018, also allow <strong>temporal queries</strong>‚Äîa crucial capability for searching user signals over time windows, such as "has user X clicked in the last 10 minutes?".
            </p>

            <h3>üöÄ Outcome: Scaling Personalized Search Efficiently</h3>
            <p>
                With Mussel and RocksDB, Airbnb achieved:
            </p>
            <ul>
                <li><strong>High ingest speed</strong> of real-time user activity</li>
                <li><strong>Low p99 latency</strong> via LSM + Bloom + Cache</li>
                <li><strong>Efficient resource usage</strong> across SSD/HDD layers</li>
            </ul>

            <blockquote>
                "RocksDB gave us the ability to shape our key-value access patterns with compaction, caching, and Bloom filter tuning‚Äîwithout rebuilding everything from scratch."
                <br><em>‚Äî Airbnb Infrastructure Team</em>
            </blockquote>
        </section>

      <section id="core-data-structures">
        <h2>üß± Section 2: Core Data Structures and Algorithms Used</h2>

        <!-- 1. LSM Tree -->
        <article id="lsm-tree">
            <h3>1. <strong>LSM Tree (Log-Structured Merge Tree)</strong></h3>

            <p>
                A <strong>Log-Structured Merge Tree</strong> (LSM Tree) is a write-optimized hierarchical structure designed for high-ingest systems like databases and key-value stores. Instead of writing directly to disk for every update (as B-Trees do), LSM Trees accumulate writes in memory and periodically flush them to disk in bulk. This transforms many small random writes into fewer sequential writes ‚Äî a massive win on SSDs.
            </p>

            <p>
                The tree consists of a memory component (C0) and multiple disk levels (C1, C2...). Each level holds larger, sorted files, and the system periodically merges files across levels through a process called <strong>compaction</strong>.
            </p>

            <figure>
                <img src="/assets/LSM_tree.png" alt="LSM Tree Architecture" width="650">
                <figcaption><strong>Figure 1:</strong> LSM Tree layers: writes enter C0 (in-memory) and flush to sorted disk-based levels (C1, C2...) via compaction. Source: Wikipedia</figcaption>
                <p><a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree" target="_blank">Source: Log-structured Merge Tree</a></p>
            </figure>

            <h4>üõ† How RocksDB Uses It</h4>
            <p>
                RocksDB is a high-performance key-value engine built around the LSM Tree model. New data enters a MemTable in RAM. Once full, it is flushed to disk as an SSTable (Sorted String Table). These SSTables are compacted into progressively larger levels, maintaining sorted order.
            </p>
            <p>
                Airbnb's Mussel system, based on RocksDB, uses this architecture to ingest over <strong>1 million signals per second</strong> ‚Äî from user views and clicks to host updates ‚Äî without compromising read performance.
            </p>

            <h4>‚è± Time Complexities</h4>
            <table>
                <thead>
                    <tr><th>Operation</th><th>Complexity</th></tr>
                </thead>
                <tbody>
                    <tr><td>Insert</td><td>Amortized <strong>O(log N)</strong></td></tr>
                    <tr><td>Read</td><td><strong>O(log L √ó log N)</strong> (L = SSTable levels)</td></tr>
                    <tr><td>Delete</td><td><strong>O(log N)</strong> (via tombstone markers)</td></tr>
                </tbody>
            </table>
        </article>

        <!-- 2. Bloom Filters -->
        <article id="bloom-filters">
            <h3>2. <strong>Bloom Filters</strong></h3>

            <p>
                A <strong>Bloom Filter</strong> is a probabilistic data structure that answers: "Could this key exist?" It gives fast lookups in constant time using multiple hash functions and a compact bit array. The trade-off? It might say "yes" when the answer is "no" (false positives), but it will never miss a key that exists.
            </p>

            <p>
                RocksDB uses Bloom Filters with every SSTable to avoid unnecessary disk reads. When looking up a key, the engine queries the Bloom filter first. If the filter says "definitely not here," the SSTable is skipped entirely ‚Äî saving costly disk I/O.
            </p>

            <figure>
                <img src="assets/Bloom_filter.png" alt="Bloom Filter Diagram" width="600">
                <figcaption><strong>Figure 2:</strong> Bloom filters use multiple hashes to set bits in a bit array. All bits must be 1 to indicate potential membership. Source: Wikipedia</figcaption>
                <p><a href="https://en.wikipedia.org/wiki/Bloom_filter" target="_blank">Source: Bloom Filter</a></p>
            </figure>

            <h4>‚è± Time & Space Complexity</h4>
            <ul>
                <li><strong>Query:</strong> O(k) where k = number of hash functions</li>
                <li><strong>Space:</strong> O(n log(1/Œµ)) for n elements, Œµ = false positive rate</li>
            </ul>
        </article>

        <!-- 3. MemTable -->
        <article id="memtable">
            <h3>3. <strong>MemTable (Write Buffer)</strong></h3>

            <p>
                A <strong>MemTable</strong> is the first stop for new writes. It's an in-memory data structure ‚Äî typically a <em>Skip List</em> or <em>Red-Black Tree</em> ‚Äî that temporarily stores key-value pairs. Once it fills up, it flushes its contents to disk as an immutable SSTable.
            </p>

            <p>
                In RocksDB, this buffering reduces the cost of disk writes and allows compactions to batch multiple changes efficiently. While active, MemTables also serve recent reads ‚Äî ensuring low-latency access to fresh data.
            </p>

            <h4>‚è± Time Complexities</h4>
            <ul>
                <li><strong>Insert/Search/Delete:</strong> O(log n)</li>
                <li><strong>Flush:</strong> O(n), written sequentially to disk</li>
            </ul>
        </article>

        <!-- 4. Compaction -->
        <article id="compaction">
            <h3>4. <strong>Compaction Algorithms</strong></h3>

            <p>
                Compaction is the LSM Tree's cleanup crew. It merges SSTables across levels, removes obsolete keys (e.g., overwritten or deleted), and maintains sorted order. Without compaction, RocksDB would become a read-performance disaster due to too many SSTables.
            </p>

            <p>
                RocksDB supports multiple compaction styles:
            <ul>
                <li><strong>Leveled Compaction</strong>: Aggressively merges SSTables into fixed-size levels. Great for reads.</li>
                <li><strong>Universal Compaction</strong>: Flexible, less aggressive merging. Ideal for fast-write workloads like Airbnb's signal ingestion pipeline.</li>
                <li><strong>FIFO Compaction</strong>: Deletes old files in order ‚Äî used for time-series or log data.</li>
            </ul>
            </p>

            <figure>
                <img src="assets/Leveled_compaction.png" alt="Leveled Compaction in RocksDB" width="650">
                <figcaption><strong>Figure 3:</strong> In Leveled Compaction, newer SSTables are merged downward into larger levels. Source: RocksDB GitHub Wiki</figcaption>
                <p><a href="https://github.com/facebook/rocksdb/wiki/Memtable" target="_blank">Source: MemTable</a></p>
            </figure>

            <h4>‚è± Complexity</h4>
            <ul>
                <li><strong>Worst-case:</strong> O(N) during major compactions</li>
                <li><strong>Amortized:</strong> Lower, depending on tuning of levels and compaction triggers</li>
            </ul>
        </article>

        <!-- 5. LRU Cache -->
        <article id="lru-cache">
            <h3>5. <strong>Cache Layer (LRU Cache)</strong></h3>

            <p>
                RocksDB includes a <strong>block cache</strong> to store frequently accessed data blocks from SSTables. It uses the <strong>Least Recently Used (LRU)</strong> replacement policy ‚Äî implemented with a combination of a hash map and a doubly linked list.
            </p>

            <p>
                This is crucial for performance at Airbnb: if a user views a listing multiple times in a short period, the data stays hot in cache, avoiding repeated disk reads and improving response time.
            </p>

            <figure>
                <img src="assets/LRU_cache.png" alt="LRU Cache Mechanism" width="600">
                <figcaption><strong>Figure 4:</strong> LRU cache mechanism stores frequently accessed items at the front of a list, evicts least-recently-used ones. Source: Wikipedia</figcaption>
                <p><a href="https://en.wikipedia.org/wiki/Cache_replacement_policies" target="_blank">Source: Cache Replacement Policies</a></p>
            </figure>

            <h4>‚è± Time Complexity</h4>
            <ul>
                <li><strong>Lookup:</strong> O(1)</li>
                <li><strong>Insert/Evict:</strong> O(1)</li>
            </ul>
        </article>

        <!-- Summary Table -->
        <article id="summary-table">
            <h3>üßÆ Summary: Time and Space Complexity Table</h3>
            <table>
                <thead>
                    <tr>
                        <th>Component</th>
                        <th>Data Structure</th>
                        <th>Operation</th>
                        <th>Time Complexity</th>
                        <th>Purpose in RocksDB</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>MemTable</td>
                        <td>Skip List / RB Tree</td>
                        <td>Insert/Search</td>
                        <td><strong>O(log n)</strong></td>
                        <td>Fast, in-memory write buffer</td>
                    </tr>
                    <tr>
                        <td>SSTable</td>
                        <td>Sorted Array</td>
                        <td>Binary Search</td>
                        <td><strong>O(log n)</strong></td>
                        <td>On-disk immutable key-value store</td>
                    </tr>
                    <tr>
                        <td>Bloom Filter</td>
                        <td>Bit Array + Hashing</td>
                        <td>Membership Check</td>
                        <td><strong>O(k)</strong></td>
                        <td>Skips unnecessary disk I/O</td>
                    </tr>
                    <tr>
                        <td>Compaction</td>
                        <td>Sorted Merge</td>
                        <td>Merge N keys</td>
                        <td><strong>O(N)</strong></td>
                        <td>Reduces fragmentation & disk usage</td>
                    </tr>
                    <tr>
                        <td>Block Cache</td>
                        <td>HashMap + DLL (LRU)</td>
                        <td>Access/Evict</td>
                        <td><strong>O(1)</strong></td>
                        <td>Improves read performance</td>
                    </tr>
                </tbody>
            </table>
        </article>
    </section>

        <section id="efficiency-tradeoffs">
            <div class="content-wrapper">
                <h2><span class="emoji">‚öôÔ∏è</span>Section 3: Time Efficiency & Tradeoffs</h2>
                <p>
                    Building a high-performance key-value store like Airbnb's <strong>Mussel</strong> means making deliberate tradeoffs between write speed, read latency, memory usage, and compaction cost. Each of the data structures introduced earlier (LSM Tree, Bloom Filter, MemTable, etc.) contributes to specific performance characteristics.
                </p>
                
                <h3><span class="emoji">üìä</span>Before RocksDB (Legacy Systems)</h3>
                <ul>
                    <li><strong>Writes:</strong> MySQL and DynamoDB performed random I/O and incurred locking overhead ‚Üí poor throughput under heavy writes.</li>
                    <li><strong>Reads:</strong> Tail latencies (p95/p99) spiked under concurrent traffic. Reads were slower due to frequent disk I/O.</li>
                    <li><strong>Scalability:</strong> Needed horizontal scaling via more DB instances ‚Üí increased cost and operational complexity.</li>
                </ul>
                
                <h3><span class="emoji">üöÄ</span>After RocksDB + Mussel</h3>
                <ul>
                    <li><strong>Writes:</strong> Batched via MemTables, flushed to disk as sorted files ‚Üí high throughput</li>
                    <li><strong>Reads:</strong> Bloom filters and LRU cache reduced tail latencies dramatically</li>
                    <li><strong>Disk usage:</strong> Compaction reduced fragmentation and deleted stale keys</li>
                    <li><strong>Scalability:</strong> Embedded RocksDB engines at service boundaries eliminated the need for separate DB clusters</li>
                </ul>
                
                <h3><span class="emoji">üß†</span>Component-by-Component Tradeoffs</h3>
                <table>
                    <thead>
                        <tr>
                            <th>Component</th>
                            <th>Benefit</th>
                            <th>Tradeoff</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>LSM Tree</strong></td>
                            <td>Fast writes (sequential, batched)</td>
                            <td>Slower reads unless mitigated with filters or caches</td>
                        </tr>
                        <tr>
                            <td><strong>Bloom Filter</strong></td>
                            <td>Reduces unnecessary disk reads</td>
                            <td>Uses extra memory; allows false positives</td>
                        </tr>
                        <tr>
                            <td><strong>MemTable</strong></td>
                            <td>In-memory buffering for fast ingest</td>
                            <td>Risk of data loss if not flushed and crash occurs</td>
                        </tr>
                        <tr>
                            <td><strong>Compaction</strong></td>
                            <td>Keeps disk clean and sorted</td>
                            <td>CPU and I/O intensive during heavy merges</td>
                        </tr>
                        <tr>
                            <td><strong>LRU Cache</strong></td>
                            <td>Speeds up hot reads</td>
                            <td>Consumes significant memory under high traffic</td>
                        </tr>
                    </tbody>
                </table>
                
                <h3><span class="emoji">üìâ</span>Real Performance Gains</h3>
                <p>
                    According to Airbnb's Mussel blog and internal benchmarks:
                </p>
                
                <div class="performance-stats">
                    <div class="stat-card">
                        <span class="stat-number">1M+</span>
                        <span class="stat-label">user events/sec ingested with low CPU overhead</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-number">&lt;100ms</span>
                        <span class="stat-label">p99 latency for personalized search lookups</span>
                    </div>
                    <div class="stat-card">
                        <span class="stat-number">30-40%</span>
                        <span class="stat-label">less disk usage after compaction vs legacy HBase</span>
                    </div>
                </div>
                
                <figure>
                    <img src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*0S_F2I-5mDwXTw0Jj8UOqQ.png" alt="Airbnb Mussel Read Latency Chart" width="650">
                    <figcaption>
                        <strong>Figure 6:</strong> Tail latency improvement after switching to RocksDB in Mussel. Source: Airbnb Engineering Blog
                    </figcaption>
                </figure>
            </div>
        </section>

        <div class="mussel-performance">
            <h3>Adoption and Performance of Mussel</h3>
            <p>
                Mussel has become a core component of Airbnb‚Äôs data infrastructure, supporting a wide range of services that rely on key-value storage. It serves as the backbone for applications requiring reliable, low-latency access to derived data.
            </p>
            <ul>
                <li><strong>130TB of data:</strong> Mussel manages approximately 130TB of data across 4,000 tables in its production environment.</li>
                <li><strong>>99.9% availability:</strong> Ensures minimal downtime, providing a consistent and reliable user experience.</li>
                <li><strong>800,000 QPS:</strong> Handles an impressive read query rate, making it highly suitable for read-heavy workloads.</li>
                <li><strong>35,000 write QPS:</strong> Supports real-time updates alongside bulk data loads.</li>
                <li><strong>P95 read latency:</strong> Achieves an average P95 read latency of less than 8 milliseconds.</li>
            </ul>
            <p>
                Source: <a href="https://blog.bytebytego.com/p/how-airbnb-built-a-key-value-store" target="_blank">ByteByteGo Blog</a>
            </p>
        </div>

        <section id="references">
    <h2>üìö Section 8: References & Resources</h2>
    <h3>üìò Foundational Papers</h3>
    <ul>
        <li>
            O'Neil, P. et al. (1996). 
            <em><a href="https://www.cs.umb.edu/~poneil/lsmtree.pdf" target="_blank">
                The Log-Structured Merge-Tree (LSM-Tree)</a></em>, Acta Informatica.
        </li>
        <li>
            Dong, Q. et al. (2021). 
            <em><a href="https://dl.acm.org/doi/abs/10.1145/3453474" target="_blank">
                RocksDB: Evolution of Development Priorities</a></em>, ACM Transactions on Storage.
        </li>
        <li>
            Peng, D. et al. (2018). 
            <em><a href="https://users.cs.utah.edu/~lifeifei/papers/pbf.pdf" target="_blank">
                Persistent Bloom Filter: Membership Testing for the Entire History</a></em>, SIGMOD.
        </li>
    </ul>
    <h3>üîß Official Documentation & Source Code</h3>
    <ul>
        <li>
            <a href="https://github.com/facebook/rocksdb/wiki/MemTable" target="_blank">
                RocksDB Wiki ‚Äì MemTable</a>
        </li>
        <li>
            <a href="https://github.com/facebook/rocksdb/wiki/Compaction" target="_blank">
                RocksDB Wiki ‚Äì Compaction</a>
        </li>
        <li>
            <a href="https://github.com/facebook/rocksdb/wiki/universal-compaction" target="_blank">
                RocksDB Wiki ‚Äì Universal Compaction</a>
        </li>
        <li>
            <a href="https://rocksdb.org/blog/" target="_blank">
                RocksDB Official Blog</a>
        </li>
    </ul>
    <h3>üõ† Airbnb Engineering Case Material</h3>
    <ul>
        <li>
            <a href="https://medium.com/airbnb-engineering/mussel-airbnbs-key-value-store-for-derived-data-406b9fa1b296" target="_blank">
                Mussel: Airbnb's Key-Value Store for Derived Data</a>
        </li>
        <li>
            <a href="https://medium.com/airbnb-engineering/building-a-user-signals-platform-at-airbnb-b236078ec82b" target="_blank">
                Building a User Signals Platform at Airbnb</a>
        </li>
    </ul>
    <h3>üìä System Design Blogs & Visual Deep-Dives</h3>
    <ul>
        <li>
            <a href="https://blog.bytebytego.com/p/how-airbnb-built-a-key-value-store" target="_blank">
                How Airbnb Built a Key-Value Store for Petabytes of Data ‚Äì ByteByteGo</a>
        </li>
        <li>
            <a href="https://blog.bytebytego.com/p/how-airbnb-powers-personalization" target="_blank">
                How Airbnb Powers Personalization ‚Äì ByteByteGo</a>
        </li>
        <li>
            <a href="https://www.alibabacloud.com/blog/an-in-depth-discussion-on-the-lsm-compaction-mechanism_596780" target="_blank">
                LSM Compaction Deep Dive ‚Äì Alibaba Cloud Blog</a>
        </li>
    </ul>
    <h3>üéôÔ∏è Talks & Slides</h3>
    <ul>
        <li>
            <a href="https://www.youtube.com/watch?v=ASQ6XMtogMs" target="_blank">
                RocksDB for Personalized Search at Airbnb ‚Äì Tao Xu (YouTube)</a>
        </li>
        <li>
            <a href="https://plus-archive.qconferences.com/speakers/jessica-tai-0" target="_blank">
                QCon: Migrating Airbnb's Monolith to Mussel ‚Äì Jessica Tai</a>
        </li>
    </ul>
    <h3>üìà Benchmarks & Tuning Guides</h3>
    <ul>
        <li>
            <a href="https://smalldatum.blogspot.com/2023/06/universal-compaction-in-rocksdb-and-me.html" target="_blank">
                Small Datum: Universal Compaction in RocksDB</a>
        </li>
        <li>
            <a href="https://smalldatum.blogspot.com/2024/11/rocksdb-benchmarks-large-server.html" target="_blank">
                Small Datum: RocksDB Benchmarks on Large Servers</a>
        </li>
        <li>
            <a href="https://www.confluent.io/blog/how-to-tune-rocksdb-kafka-streams-state-stores-performance/" target="_blank">
                Tuning RocksDB for Kafka Streams ‚Äì Confluent Blog</a>
        </li>
    </ul>
    <h3>üß™ Open Datasets & Benchmark Tools</h3>
    <ul>
        <li><strong>YCSB Workloads A‚ÄìF</strong>: Standard key-value store performance workloads</li>
        <li><strong>RocksDB db_bench Utility</strong>: Built-in RocksDB benchmarking tool</li>
        <li><strong>CloudLab SSD I/O Traces</strong>: For modeling compaction on SSD vs HDD</li>
    </ul>
</section>
    </div>
</body>

</html>





